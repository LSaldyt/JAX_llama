{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from llama import flax_model, model\n",
    "import torch\n",
    "import numpy as np\n",
    "from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
    "from typing import Tuple, List, Tuple\n",
    "import os\n",
    "from flax.core.frozen_dict import unfreeze, freeze\n",
    "from dataclasses import dataclass, asdict\n",
    "import functools\n",
    "from example import load\n",
    "from convert_weights import convert_llama_weights\n",
    "from llama.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "ckpt_dir = '/shared/csnell/llama_weights/7B/'\n",
    "tokenizer_path = '/shared/csnell/llama_weights/tokenizer.model'\n",
    "tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "# params = {\"dim\": 4096, \"multiple_of\": 256, \"n_heads\": 32, \"n_layers\": 32, \"norm_eps\": 1e-06, \"vocab_size\": tokenizer.n_words, \"max_batch_size\": 32, \"max_seq_len\": 1024}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax_model = flax_model.Transformer(**params)\n",
    "# jax_state = jax_model.init(\n",
    "#     jax.random.PRNGKey(0), \n",
    "#     jnp.ones((1, 1), dtype=jnp.int32), \n",
    "#     0, \n",
    "# )\n",
    "jax_weights, params = convert_llama_weights(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({'max_batch_size': 32, 'max_seq_len': 1024, 'vocab_size': tokenizer.n_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_state = {\n",
    "    'params': jax_weights, \n",
    "    'cache': {\n",
    "        'layer_%d' % (i): {\n",
    "            'attention': {\n",
    "                'cache_k': jnp.zeros((params['max_batch_size'], params['max_seq_len'], params['n_heads'], params['dim'] // params['n_heads'])), \n",
    "            }, \n",
    "        } for i in range(params['n_layers'])\n",
    "    }, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 450, 7483, 310, 9556, 338, 278, 4272, 310],\n",
       " [1,\n",
       "  2266,\n",
       "  338,\n",
       "  590,\n",
       "  1487,\n",
       "  1212,\n",
       "  297,\n",
       "  278,\n",
       "  3114,\n",
       "  310,\n",
       "  23688,\n",
       "  1048,\n",
       "  385,\n",
       "  23116,\n",
       "  21082,\n",
       "  29901]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_strs = [\n",
    "    \"The capital of Germany is the city of\", \n",
    "    \"Here is my sonnet in the style of Shakespeare about an artificial intelligence:\", \n",
    "]\n",
    "tokens = [tokenizer.encode(x, bos=True, eos=False) for x in test_strs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_model = flax_model.Transformer(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 19:05:54.701329: W external/org_tensorflow/tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 512.00MiB (rounded to 536870912)requested by op \n",
      "2023-03-06 19:05:54.701721: W external/org_tensorflow/tensorflow/tsl/framework/bfc_allocator.cc:497] *********************************************_******************************************************\n",
      "2023-03-06 19:05:54.702089: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2410] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 536870912 bytes.\n",
      "BufferAssignment OOM Debugging.\n",
      "BufferAssignment stats:\n",
      "             parameter allocation:  512.14MiB\n",
      "              constant allocation:        16B\n",
      "        maybe_live_out allocation:  512.00MiB\n",
      "     preallocated temp allocation:   144.0KiB\n",
      "  preallocated temp fragmentation:         0B (0.00%)\n",
      "                 total allocation:    1.00GiB\n",
      "Peak buffers:\n",
      "\tBuffer 1:\n",
      "\t\tSize: 512.00MiB\n",
      "\t\tEntry Parameter Subshape: f32[32,1024,32,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 2:\n",
      "\t\tSize: 512.00MiB\n",
      "\t\tXLA Label: copy\n",
      "\t\tShape: f32[32,1024,32,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 3:\n",
      "\t\tSize: 144.0KiB\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: f32[1,9,32,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 4:\n",
      "\t\tSize: 144.0KiB\n",
      "\t\tEntry Parameter Subshape: f32[1,9,32,128]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 5:\n",
      "\t\tSize: 16B\n",
      "\t\tXLA Label: constant\n",
      "\t\tShape: s32[4]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 6:\n",
      "\t\tSize: 8B\n",
      "\t\tEntry Parameter Subshape: s32[2]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 7:\n",
      "\t\tSize: 1B\n",
      "\t\tXLA Label: fusion\n",
      "\t\tShape: pred[]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 8:\n",
      "\t\tSize: 1B\n",
      "\t\tXLA Label: parameter\n",
      "\t\tShape: pred[]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 9:\n",
      "\t\tSize: 1B\n",
      "\t\tXLA Label: parameter\n",
      "\t\tShape: pred[]\n",
      "\t\t==========================\n",
      "\n",
      "\tBuffer 10:\n",
      "\t\tSize: 1B\n",
      "\t\tXLA Label: and\n",
      "\t\tShape: pred[]\n",
      "\t\t==========================\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 536870912 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  512.14MiB\n              constant allocation:        16B\n        maybe_live_out allocation:  512.00MiB\n     preallocated temp allocation:   144.0KiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:    1.00GiB\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 512.00MiB\n\t\tEntry Parameter Subshape: f32[32,1024,32,128]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 512.00MiB\n\t\tXLA Label: copy\n\t\tShape: f32[32,1024,32,128]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 144.0KiB\n\t\tXLA Label: fusion\n\t\tShape: f32[1,9,32,128]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 144.0KiB\n\t\tEntry Parameter Subshape: f32[1,9,32,128]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 16B\n\t\tXLA Label: constant\n\t\tShape: s32[4]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 8B\n\t\tEntry Parameter Subshape: s32[2]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 1B\n\t\tXLA Label: fusion\n\t\tShape: pred[]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 1B\n\t\tXLA Label: parameter\n\t\tShape: pred[]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 1B\n\t\tXLA Label: parameter\n\t\tShape: pred[]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 1B\n\t\tXLA Label: and\n\t\tShape: pred[]\n\t\t==========================\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m jax_model\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m      2\u001b[0m     jax_state, \n\u001b[1;32m      3\u001b[0m     jnp\u001b[39m.\u001b[39;49masarray(tokens[\u001b[39m0\u001b[39;49m][:params[\u001b[39m'\u001b[39;49m\u001b[39mmax_seq_len\u001b[39;49m\u001b[39m'\u001b[39;49m]])[\u001b[39mNone\u001b[39;49;00m], \n\u001b[1;32m      4\u001b[0m     \u001b[39m0\u001b[39;49m, \n\u001b[1;32m      5\u001b[0m     mutable\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mcache\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m      6\u001b[0m )\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/JAX_llama/llama/flax_model.py:272\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, tokens, start_pos)\u001b[0m\n\u001b[1;32m    269\u001b[0m     mask \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mtriu(mask, k\u001b[39m=\u001b[39mstart_pos\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    271\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 272\u001b[0m     h \u001b[39m=\u001b[39m layer(h, start_pos, freqs_cis, mask)\n\u001b[1;32m    273\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(h)\n\u001b[1;32m    274\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(h[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])  \u001b[39m# only compute last logits\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/JAX_llama/llama/flax_model.py:209\u001b[0m, in \u001b[0;36mTransformerBlock.__call__\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x: jnp\u001b[39m.\u001b[39mndarray, start_pos: \u001b[39mint\u001b[39m, freqs_cis: jnp\u001b[39m.\u001b[39mndarray, mask: Optional[jnp\u001b[39m.\u001b[39mndarray]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m jnp\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m--> 209\u001b[0m     h \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_norm(x), start_pos, freqs_cis, mask)\n\u001b[1;32m    210\u001b[0m     out \u001b[39m=\u001b[39m h \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn_norm(h))\n\u001b[1;32m    211\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/JAX_llama/llama/flax_model.py:117\u001b[0m, in \u001b[0;36mAttention.__call__\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m    113\u001b[0m xv \u001b[39m=\u001b[39m xv\u001b[39m.\u001b[39mreshape(bsz, seqlen, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[1;32m    115\u001b[0m xq, xk \u001b[39m=\u001b[39m apply_rotary_emb(xq, xk, freqs_cis\u001b[39m=\u001b[39mfreqs_cis)\n\u001b[0;32m--> 117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_k\u001b[39m.\u001b[39mvalue \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_k\u001b[39m.\u001b[39;49mvalue\u001b[39m.\u001b[39;49mat[:bsz, start_pos:(start_pos\u001b[39m+\u001b[39;49mseqlen)]\u001b[39m.\u001b[39;49mset(xk)\n\u001b[1;32m    118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_v\u001b[39m.\u001b[39mvalue \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_v\u001b[39m.\u001b[39mvalue\u001b[39m.\u001b[39mat[:bsz, start_pos:(start_pos\u001b[39m+\u001b[39mseqlen)]\u001b[39m.\u001b[39mset(xv)\n\u001b[1;32m    120\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache_k\u001b[39m.\u001b[39mvalue[:bsz, :(start_pos\u001b[39m+\u001b[39mseqlen)]\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.9/site-packages/jax/_src/numpy/lax_numpy.py:5375\u001b[0m, in \u001b[0;36m_IndexUpdateRef.set\u001b[0;34m(self, values, indices_are_sorted, unique_indices, mode)\u001b[0m\n\u001b[1;32m   5366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset\u001b[39m(\u001b[39mself\u001b[39m, values, indices_are_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, unique_indices\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   5367\u001b[0m         mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   5368\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Pure equivalent of ``x[idx] = y``.\u001b[39;00m\n\u001b[1;32m   5369\u001b[0m \n\u001b[1;32m   5370\u001b[0m \u001b[39m  Returns the value of ``x`` that would result from the NumPy-style\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5373\u001b[0m \u001b[39m  See :mod:`jax.ops` for details.\u001b[39;00m\n\u001b[1;32m   5374\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5375\u001b[0m   \u001b[39mreturn\u001b[39;00m scatter\u001b[39m.\u001b[39;49m_scatter_update(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex, values, lax\u001b[39m.\u001b[39;49mscatter,\n\u001b[1;32m   5376\u001b[0m                                  indices_are_sorted\u001b[39m=\u001b[39;49mindices_are_sorted,\n\u001b[1;32m   5377\u001b[0m                                  unique_indices\u001b[39m=\u001b[39;49munique_indices, mode\u001b[39m=\u001b[39;49mmode)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.9/site-packages/jax/_src/ops/scatter.py:71\u001b[0m, in \u001b[0;36m_scatter_update\u001b[0;34m(x, idx, y, scatter_op, indices_are_sorted, unique_indices, mode, normalize_indices)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m# XLA gathers and scatters are very similar in structure; the scatter logic\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m# is more or less a transpose of the gather equivalent.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m treedef, static_idx, dynamic_idx \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39m_split_index_for_jit(idx, x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 71\u001b[0m \u001b[39mreturn\u001b[39;00m _scatter_impl(x, y, scatter_op, treedef, static_idx, dynamic_idx,\n\u001b[1;32m     72\u001b[0m                      indices_are_sorted, unique_indices, mode,\n\u001b[1;32m     73\u001b[0m                      normalize_indices)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.9/site-packages/jax/_src/ops/scatter.py:117\u001b[0m, in \u001b[0;36m_scatter_impl\u001b[0;34m(x, y, scatter_op, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, normalize_indices)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m# Transpose the gather dimensions into scatter dimensions (cf.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# lax._gather_transpose_rule)\u001b[39;00m\n\u001b[1;32m    112\u001b[0m dnums \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mScatterDimensionNumbers(\n\u001b[1;32m    113\u001b[0m   update_window_dims\u001b[39m=\u001b[39mindexer\u001b[39m.\u001b[39mdnums\u001b[39m.\u001b[39moffset_dims,\n\u001b[1;32m    114\u001b[0m   inserted_window_dims\u001b[39m=\u001b[39mindexer\u001b[39m.\u001b[39mdnums\u001b[39m.\u001b[39mcollapsed_slice_dims,\n\u001b[1;32m    115\u001b[0m   scatter_dims_to_operand_dims\u001b[39m=\u001b[39mindexer\u001b[39m.\u001b[39mdnums\u001b[39m.\u001b[39mstart_index_map\n\u001b[1;32m    116\u001b[0m )\n\u001b[0;32m--> 117\u001b[0m out \u001b[39m=\u001b[39m scatter_op(\n\u001b[1;32m    118\u001b[0m   x, indexer\u001b[39m.\u001b[39;49mgather_indices, y, dnums,\n\u001b[1;32m    119\u001b[0m   indices_are_sorted\u001b[39m=\u001b[39;49mindexer\u001b[39m.\u001b[39;49mindices_are_sorted \u001b[39mor\u001b[39;49;00m indices_are_sorted,\n\u001b[1;32m    120\u001b[0m   unique_indices\u001b[39m=\u001b[39;49mindexer\u001b[39m.\u001b[39;49munique_indices \u001b[39mor\u001b[39;49;00m unique_indices,\n\u001b[1;32m    121\u001b[0m   mode\u001b[39m=\u001b[39;49mmode)\n\u001b[1;32m    122\u001b[0m \u001b[39mreturn\u001b[39;00m lax_internal\u001b[39m.\u001b[39m_convert_element_type(out, dtype, weak_type)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.9/site-packages/jax/_src/interpreters/pxla.py:2166\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   2164\u001b[0m   out_bufs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_with_tokens(input_bufs)\n\u001b[1;32m   2165\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2166\u001b[0m   out_bufs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxla_executable\u001b[39m.\u001b[39;49mexecute_sharded_on_local_devices(\n\u001b[1;32m   2167\u001b[0m       input_bufs\n\u001b[1;32m   2168\u001b[0m   )\n\u001b[1;32m   2169\u001b[0m \u001b[39mif\u001b[39;00m dispatch\u001b[39m.\u001b[39mneeds_check_special():\n\u001b[1;32m   2170\u001b[0m   \u001b[39mfor\u001b[39;00m bufs \u001b[39min\u001b[39;00m out_bufs:\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 536870912 bytes.\nBufferAssignment OOM Debugging.\nBufferAssignment stats:\n             parameter allocation:  512.14MiB\n              constant allocation:        16B\n        maybe_live_out allocation:  512.00MiB\n     preallocated temp allocation:   144.0KiB\n  preallocated temp fragmentation:         0B (0.00%)\n                 total allocation:    1.00GiB\nPeak buffers:\n\tBuffer 1:\n\t\tSize: 512.00MiB\n\t\tEntry Parameter Subshape: f32[32,1024,32,128]\n\t\t==========================\n\n\tBuffer 2:\n\t\tSize: 512.00MiB\n\t\tXLA Label: copy\n\t\tShape: f32[32,1024,32,128]\n\t\t==========================\n\n\tBuffer 3:\n\t\tSize: 144.0KiB\n\t\tXLA Label: fusion\n\t\tShape: f32[1,9,32,128]\n\t\t==========================\n\n\tBuffer 4:\n\t\tSize: 144.0KiB\n\t\tEntry Parameter Subshape: f32[1,9,32,128]\n\t\t==========================\n\n\tBuffer 5:\n\t\tSize: 16B\n\t\tXLA Label: constant\n\t\tShape: s32[4]\n\t\t==========================\n\n\tBuffer 6:\n\t\tSize: 8B\n\t\tEntry Parameter Subshape: s32[2]\n\t\t==========================\n\n\tBuffer 7:\n\t\tSize: 1B\n\t\tXLA Label: fusion\n\t\tShape: pred[]\n\t\t==========================\n\n\tBuffer 8:\n\t\tSize: 1B\n\t\tXLA Label: parameter\n\t\tShape: pred[]\n\t\t==========================\n\n\tBuffer 9:\n\t\tSize: 1B\n\t\tXLA Label: parameter\n\t\tShape: pred[]\n\t\t==========================\n\n\tBuffer 10:\n\t\tSize: 1B\n\t\tXLA Label: and\n\t\tShape: pred[]\n\t\t==========================\n\n"
     ]
    }
   ],
   "source": [
    "jax_model.apply(\n",
    "    jax_state, \n",
    "    jnp.asarray(tokens[0][:params['max_seq_len']])[None], \n",
    "    0, \n",
    "    mutable=['cache'], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1181406b754c264ee4908ca6fe876266b9ff5c1be290a8000636ee7f346a847f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
